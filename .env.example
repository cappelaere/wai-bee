
# WAI 2026 Scholarship Processing Configuration
# Copy this file to .env and update with your values

# API_SERVER_URL="http://localhost:8200"
# HOST=150.240.3.116
HOST=localhost
API_SERVER_URL="http://$HOST:8200"

MARKDOWN_PARSER=true

# User Authentication Passwords
ADMIN_PASSWORD="CHANGE_ME"
USER_PASSWORD="CHANGE_ME"
DELANEY_PASSWORD="CHANGE_ME"
EVANS_PASSWORD="CHANGE_ME"

# Multi-Tenancy Configuration
USER_CONFIG_FILE="config/users.json"
TOKEN_EXPIRY_HOURS=24
ENABLE_AUDIT_LOG=true
AUDIT_LOG_FILE="logs/access_audit.log"

# LLM Configuration
PRIMARY_MODEL=ollama/llama3.2:3b
FALLBACK_MODEL=ollama/llama3:latest
LARGE_MODEL=ollama/llama3.2:3b

# LLM API Keys
ANTHROPIC_API_KEY="REPLACE_ME"
OPENAI_API_KEY="REPLACE_ME"

# Chat Model Configuration
# Supports: Anthropic, Ollama, OpenAI, etc.
# Format: "provider:model-name"
#
# Ollama (fast, local, free - requires Ollama running):
# CHAT_MODEL="ollama/llama3.2:3b"
# CHAT_MODEL="ollama/llama3:8b"
# CHAT_MODEL="ollama/qwen2.5:7b"
#
# OpenAI:
# CHAT_MODEL="openai:gpt-4"
# CHAT_MODEL="openai:gpt-3.5-turbo"

# Orchestrator Model (for routing decisions)
# Use a faster/cheaper model for orchestrator, better model for actual work
# If not set, uses same as CHAT_MODEL
#
# Fast routing with Ollama:
# ORCHESTRATOR_MODEL="ollama/llama3.2:1b"
#
# Or use same model:
# Anthropic (high quality, slower, API cost):
# Options: llama3.2:3b, llama3:latest, qwen2.5:7b, mistral:latest
ORCHESTRATOR_MODEL="anthropic:claude-sonnet-4-20250514"
#CHAT_MODEL="anthropic:claude-sonnet-4-20250514"
CHAT_MODEL="ollama/llama3.2:1b"

# LangFuse Observability Configuration (Cloud - US Region)
# Sign up at https://cloud.langfuse.com and get your API keys
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY="REPLACE_ME"
LANGFUSE_SECRET_KEY="REPLACE_ME"
LANGFUSE_HOST="https://us.cloud.langfuse.com"

# Observability Dashboard URL (for admin access button)
OBSERVABILITY_DASHBOARD="https://us.cloud.langfuse.com/project/cmj1nwb0300o9ad08jdqobg9l"


ORCHESTRATOR_MODEL=ollama/llama3:latest

MAX_RETRIES=3
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=4000

# OpenAI Configuration (optional - if using OpenAI instead of Ollama)
# OPENAI_API_KEY=your_api_key_here

# Processing Configuration
MAX_APPLICATIONS=None
MAX_FILES_PER_FOLDER=5
SKIP_PROCESSED=false
OVERWRITE_EXISTING=true

# Parallel Processing
ENABLE_PARALLEL=true
MAX_WORKERS=3

# Directory Configuration
DATA_DIR=data
OUTPUTS_DIR=outputs
SCHEMAS_DIR=schemas
TEMPLATES_DIR=templates

# PII Redaction Configuration
PII_SCORE_THRESHOLD=0.5
PII_EXCLUDE_ENTITIES=PERSON,LOCATION,NRP

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=logs/wai_processing.log
LOG_FORMAT='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
